{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(train_doc, test_doc):\n",
    "    X_train = []\n",
    "    for each_doc in train_doc:\n",
    "        with open(each_doc, \"r\") as file:\n",
    "            X_train.append((each_doc, file.read()))\n",
    "            \n",
    "    X_test=[]\n",
    "    for each_doc in test_doc:\n",
    "        with open(each_doc, \"r\") as file:\n",
    "            X_test.append((each_doc, file.read()))\n",
    "           \n",
    "    stopwords = ['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone',\n",
    "             'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount',\n",
    "             'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around',\n",
    "             'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before',\n",
    "             'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both',\n",
    "             'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de',\n",
    "             'describe', 'detail', 'did', 'do', 'does', 'doing', 'don', 'done', 'down', 'due', 'during', 'each', 'eg',\n",
    "             'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone',\n",
    "             'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for',\n",
    "             'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had',\n",
    "             'has', 'hasnt', 'have', 'having', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed',\n",
    "             'interest', 'into', 'is', 'it', 'its', 'itself', 'just', 'keep', 'last', 'latter', 'latterly', 'least', 'less',\n",
    "             'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly',\n",
    "             'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine',\n",
    "             'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once',\n",
    "             'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own',\n",
    "             'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed', 'seeming',\n",
    "             'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', \n",
    "             'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system',\n",
    "             't', 'take', 'ten', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there',\n",
    "             'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thickv', 'thin', 'third', 'this',\n",
    "             'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward',\n",
    "             'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we',\n",
    "             'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby',\n",
    "             'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom',\n",
    "             'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself',\n",
    "             'yourselves']\n",
    "    # Building the Vocabulary of the words for the given documents\n",
    "    vocab = {}\n",
    "    for i in range(len(train_doc)):\n",
    "        word_list = []\n",
    "        for word in X_train[i][1].split():\n",
    "            # remove any punctuation and change all the words into lower case character\n",
    "            word_new = word.strip(string.punctuation).strip('\\n').strip('\\t').lower()\n",
    "#             # Using Potter Steemming\n",
    "#             ps = PorterStemmer()\n",
    "#             word_new = ps.stem(word_new)\n",
    "            # Using Lemmatization\n",
    "            wordnet_lemmatizer = WordNetLemmatizer()\n",
    "            word_new = wordnet_lemmatizer.lemmatize(word_new)\n",
    "            if (len(word_new) > 2) and (word_new not in stopwords):\n",
    "                if word_new in vocab:\n",
    "                    vocab[word_new] += 1\n",
    "                else:\n",
    "                    vocab[word_new] = 1\n",
    "    num_words = [0 for i in range(max(vocab.values())+1)]\n",
    "    frequency = [i for i in range(max(vocab.values())+1)]\n",
    "    for key in vocab:\n",
    "        num_words[vocab[key]]+=1\n",
    "        \n",
    "    cutoff_freq = 20\n",
    "    num_words_above_cutoff = len(vocab)-sum(num_words[0:cutoff_freq])\n",
    "\n",
    "    print(\"Number of words with frequency higher than cutoff frequency({}) :\".format(cutoff_freq),num_words_above_cutoff)\n",
    "    \n",
    "    features = []\n",
    "    for key in vocab:\n",
    "        if  vocab[key] >= cutoff_freq:\n",
    "            features.append(key)\n",
    "    \n",
    "    X_train_dataset = np.zeros((len(X_train), len(features)))\n",
    "    \n",
    "    # The construction of the word vector \n",
    "    for i in range(len(X_train)):\n",
    "        word_list = [ word.strip(string.punctuation).strip('\\n').strip('\\t').lower() for word in X_train[i][1].split()]\n",
    "        for word in word_list:\n",
    "            if word in features:\n",
    "                X_train_dataset[i][features.index(word)] += 1\n",
    "    \n",
    "    X_test_dataset = np.zeros((len(X_test),len(features)))\n",
    "    \n",
    "    for i in range(len(X_test)):\n",
    "        word_list = [ word.strip(string.punctuation).strip('\\n').strip('\\t').lower() for word in X_test[i][1].split()]\n",
    "        for word in word_list:\n",
    "            if word in features:\n",
    "                X_test_dataset[i][features.index(word)] += 1\n",
    "                \n",
    "    return X_train_dataset, X_test_dataset                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Naive Bayes from scratch\n",
    "class NaiveBayesModel:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # count is a dictionary which stores several dictionaries corresponding to each category\n",
    "        # each value in the subdictionary represents the freq of the key corresponding to that category \n",
    "        self.table = {}\n",
    "        \n",
    "        self.categories = None\n",
    "    \n",
    "    def fit(self,X_train,Y_train):\n",
    "        self.categories = set(Y_train)\n",
    "        for cat in self.categories:\n",
    "            self.table[cat] = {}\n",
    "            for i in range(len(X_train[0])):\n",
    "                self.table[cat][i] = 0\n",
    "            self.table[cat]['total_words'] = 0\n",
    "            self.table[cat]['total_counts'] = 0\n",
    "        self.table['total_counts'] = len(X_train)\n",
    "        \n",
    "        for i in range(len(X_train)):\n",
    "            for j in range(len(X_train[0])):\n",
    "                self.table[Y_train[i]][j]+=X_train[i][j]\n",
    "                self.table[Y_train[i]]['total_words']+=X_train[i][j]\n",
    "            self.table[Y_train[i]]['total_counts']+=1\n",
    "        \n",
    "    def calculate_prob(self,test,category):\n",
    "        # Avoid Numberical Underlow using Log probability\n",
    "        total_words = len(test)\n",
    "        # Log (A/B) = Log(A) - Log(B)\n",
    "        log_prob = np.log(self.table[category]['total_counts']) - np.log(self.table['total_counts'])\n",
    "        \n",
    "        for i in range(len(test)):\n",
    "            current_priori = test[i]*(np.log(self.table[category][i]+1) - np.log(self.table[category]['total_words']+total_words))\n",
    "            log_prob += current_priori\n",
    "        \n",
    "        return log_prob\n",
    "        \n",
    "    def predictEach(self,test):\n",
    "        \n",
    "        global_max = None\n",
    "        initialized = True\n",
    "        global_category = None\n",
    "        \n",
    "        # Prediction for each document\n",
    "        for cat in self.categories:\n",
    "            current_log_prob = self.calculate_prob(test, cat)\n",
    "            if (initialized == True) or (current_log_prob > global_max):\n",
    "                # Updating current best probability\n",
    "                global_max = current_log_prob\n",
    "                global_category = cat\n",
    "                initialized = False\n",
    "        return global_category\n",
    "  \n",
    "    def predict(self,X_test):\n",
    "        Y_prediction = []\n",
    "        for i in range(len(X_test)):\n",
    "            Y_prediction.append(self.predictEach(X_test[i]))\n",
    "        return Y_prediction\n",
    "    \n",
    "    def score(self,Y_pred,Y_test):\n",
    "        count = 0\n",
    "        for i in range(len(Y_pred)):\n",
    "            if Y_pred[i] == Y_test[i]:\n",
    "                count += 1\n",
    "        accuracy = count / len(Y_pred)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with frequency higher than cutoff frequency(20) : 2476\n",
      "Start Training...\n",
      "Predicting...\n",
      "Enter a output file path here:./test_output.txt\n",
      "Our score on testing data : 0.8668171557562077\n",
      "Classification report for testing data :-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Cri       0.89      0.82      0.85        50\n",
      "         Dis       0.94      1.00      0.97        89\n",
      "         Oth       0.75      0.60      0.67        25\n",
      "         Pol       0.90      0.79      0.84       144\n",
      "         Str       0.80      0.93      0.86       135\n",
      "\n",
      "    accuracy                           0.87       443\n",
      "   macro avg       0.86      0.83      0.84       443\n",
      "weighted avg       0.87      0.87      0.86       443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "#     if len(sys.argv) < 2:\n",
    "#         print('Please specify the path to be listed')\n",
    "#         sys.exit()\n",
    "\n",
    "#     train_path = sys.argv[1]\n",
    "#     test_path = sys.argv[2]\n",
    "#     if not os.path.isfile(test_path) or not os.path.isfile(train_path):\n",
    "#         print(\"The file you specify does not exit !\")\n",
    "#         sys.exit()\n",
    "    train_path = \"/Users/crystalwang/Desktop/Spring 2020/nlp/TC_provided/corpus1_train.labels\"\n",
    "    test_path = \"/Users/crystalwang/Desktop/Spring 2020/nlp/TC_provided/corpus1_test.list\"\n",
    "    corpus1 = \"/Users/crystalwang/Desktop/Spring 2020/nlp/TC_provided/corpus1_test.labels\"\n",
    "    \n",
    "    train_doc = []\n",
    "    train_cat = []\n",
    "    test_doc = []\n",
    "    Y_test =[]\n",
    "    \n",
    "    with open(train_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            doc, category = line.split(' ')\n",
    "            train_doc.append(doc)\n",
    "            train_cat.append(category)\n",
    "            \n",
    "    with open(corpus1, \"r\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            doc, category = line.split(' ')\n",
    "            Y_test.append(category)\n",
    "            \n",
    "    with open(test_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip('\\n')\n",
    "            test_doc.append(line)\n",
    "            \n",
    "    X_train, X_test = preprocess(train_doc, test_doc)\n",
    "    \n",
    "    model = NaiveBayesModel()\n",
    "    print(\"Start Training...\")\n",
    "    model.fit(X_train, train_cat)\n",
    "    print(\"Predicting...\")\n",
    "    Y_test_pred = model.predict(X_test)\n",
    "    our_score_test = model.score(Y_test_pred,Y_test)\n",
    "    output_path = input('Enter a output file path here:')\n",
    "    with open(output_path, \"w\") as file:\n",
    "        \n",
    "    print(\"Our score on testing data :\",our_score_test)\n",
    "    print(\"Classification report for testing data :-\")\n",
    "    print(classification_report(Y_test, Y_test_pred))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
